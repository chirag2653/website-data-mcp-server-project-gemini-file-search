# Ingestion Pipeline Test Scope - Clear Definition

## âœ… What Ingestion Pipeline Does

**Ingestion Pipeline (Steps 1-9):**
1. Check if website exists (if yes, switch to sync)
2. Create Gemini File Search store
3. Create website record in Supabase
4. Create ingestion process job
5. Map website (discover URLs via FireCrawl)
6. Filter URLs by exact domain
7. Start batch scrape job
8. Wait for batch scrape completion (with progress logging)
9. Process results:
   - Validate completeness (URL + non-empty markdown)
   - Write complete pages to DB with `status='processing'`
   - Discard incomplete pages (never write to DB)
10. Update ingestion job: `status='completed'`
11. Update website: `last_full_crawl`
12. **Trigger indexing pipeline** â† **INGESTION ENDS HERE**

## ğŸ¯ Ingestion Pipeline Ends

**Ingestion ends after Step 12: Triggering indexing pipeline**

```typescript
// Step 9: Trigger indexing pipeline (FINAL STEP)
await indexingService.indexWebsite(website.id, {
  ingestionJobId: ingestionJob.id,
});

// Return result (ingestion complete)
return {
  websiteId: website.id,
  pagesDiscovered: discoveredUrls.length,
  pagesIndexed: pagesWritten, // Pages WRITTEN (not uploaded to Gemini)
};
```

**Key Point:** Ingestion triggers indexing but does NOT wait for it to complete. Indexing is a separate pipeline.

## ğŸ“Š What Ingestion Test Verifies

### âœ… Success Criteria for Ingestion:

1. **Website Created**
   - âœ… New website record exists
   - âœ… Gemini store created and linked
   - âœ… Store ID stored in website record

2. **URLs Discovered**
   - âœ… URLs found via FireCrawl map
   - âœ… URLs filtered by exact domain
   - âœ… Count matches process job `urls_discovered`

3. **Pages Written to DB**
   - âœ… Only complete pages written (valid URL + non-empty markdown)
   - âœ… All pages have `status='processing'` (NOT 'active')
   - âœ… All pages have `content_hash`
   - âœ… All pages have `markdown_content` (non-empty)
   - âœ… All pages have metadata

4. **No Incomplete Pages**
   - âœ… Zero pages with empty markdown
   - âœ… Zero pages missing content hash
   - âœ… All pages have required fields

5. **Process Job Tracking**
   - âœ… Ingestion job status = 'completed'
   - âœ… `urls_discovered` = URLs found
   - âœ… `urls_updated` = Pages written to DB
   - âœ… `firecrawl_batch_ids` array populated

6. **Indexing Triggered**
   - âœ… Indexing pipeline was called (can verify by checking for indexing job)

### âŒ NOT Part of Ingestion Test:

- Pages with `status='active'` (that's indexing pipeline)
- Pages with `gemini_file_id` (that's indexing pipeline)
- Indexing completion (that's separate pipeline)
- Waiting for indexing to finish (not part of ingestion)

## ğŸ”„ Two Separate Pipelines

### Ingestion Pipeline (What We're Testing)
```
Map â†’ Scrape â†’ Write 'processing' â†’ Trigger Indexing â†’ END
Duration: ~5-10 minutes (batch scrape)
Output: Pages with status='processing' in DB
```

### Indexing Pipeline (Separate, Triggered by Ingestion)
```
Pick 'processing' pages â†’ Upload to Gemini â†’ Set 'active' â†’ END
Duration: ~1-5 minutes (depends on page count)
Output: Pages with status='active' and gemini_file_id
```

## ğŸ“ Test Expectations

**After ingestion completes:**
- âœ… Pages in DB with `status='processing'`
- âœ… All pages have complete data (markdown, hash, metadata)
- âœ… No incomplete pages in DB
- âœ… Ingestion job status = 'completed'
- âœ… Indexing pipeline was triggered (separate process)

**NOT Expected:**
- âŒ Pages with `status='active'` (indexing hasn't finished yet)
- âŒ Pages with `gemini_file_id` (indexing hasn't finished yet)

## ğŸ¯ Clear Test Boundaries

**Ingestion Test = Test Steps 1-12 (up to and including triggering indexing)**
**Indexing Test = Separate test (not part of ingestion test)**

